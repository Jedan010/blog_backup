---
title: 深度学习入门
tags:
	- 深度学习
	- 神经网络
categories:
	- 深度学习

---
# 深度学习入门

经过一段时间对深入学习的学习，写一些东西来记录一下自己的所学。

## 神经网络与深度学习

深度学习是机器学习拉出的分支，它从经验中学习，并根据层次化的概念体系来理解世界，而每个概念则通过与某些相对简单的概念之间的关系来定义。

由于是从经验中学习，所以可以极大地避免由人类给计算机形式化指定它所需要学习的东西，这就能让机器自动学习，极大地促进了机器学习的发展。而层次化的概念则让机器可以通过构建简单的概念来学习复杂的概念，提高了机器学习的能力。大致地说，深度学习的深在于它的层，层越多，则越深。

现在最成功的深度学习方式是运用神经网络进行学习。神经网络也称人工神经网络（ANN）是一种模仿生物神经网络的结构和功能的数学模型或计算模型，能够从观测到的数据中进行学习。

我现在刚学习到的深度学习主要是三种神经网络：深度网络（DNN）、卷积神经网络（CNN）和循环神经网络（RNN）。

本篇文章先大致介绍一下我对深度网络的认识。

## 深度神经网络
深度神经网络（deep neural network）是现在最简单的神经网络，同时也是现在所有网络的根本，现在各式各样的神经网络都是以深度神经网络为基础发展出来的。

深度神经网络也称为前反馈神经网络（feedforward neural network），它的目标是近似某个函数$f$，使得对于所有的输入的$x$通过$f(x;\theta)$函数映射后尽可能与$y$相同。

所以，神经网络就是通过对$\theta$的学习，尽可能使得对于输入$x$映射得到的$y^{'}$与正确的$y$尽可能相同。

神经网络之所以称为网络是因为他们通常用不同的函数复合在一起表示。如$f(x)=f^{(3)}(f^{(2)}(f^{(1)}(x)))$，神经网络就是这样有很多层链接在一起，层的个数为网络的深度。神经网络的第一层是输入层，最后一层是输出层，由于训练数据没有给出中间层的输出，所以中间层的都称为隐藏层(hidden layer)。神经网络中的每一层都是由一个或多个神经元组成。

![神经网络架构](http://wiki.jikexueyuan.com/project/neural-networks-and-deep-learning-zh-cn/images/162.png)

### 感知器
让我从神经网络最基本的单元——神经元说起，最简单的人工神经元是感知器（perception)。
感知器通过对多个输入进行加权求和进行感知，判断是否达到阈值，若超过阈值，则神经元激活，否则不激活。转换一下写成公式就是判断$z$是否大于零。
$$ z=\sum_{i}w_{i}x_{i} + b $$

比如，我今天下午是否去上课，会考虑今天下午是否有课，下午的课是否重要，上课老师是否漂亮等因素，然后对这些进行加权求和与心理阈值进行比较，如果大于则输入1，去上课；否则输入0，不去上课。

通过改变权重$w$和偏置$b$，就能对输出的结果进行调整。所以，$w$和$b$是学习的关键。神经网络就是通过不断地修改$w$和$b$以实现$y^{'}$与$y$尽可能相同的目标。

当然这里还有一个重要的东西是激活函数$\sigma(z)$，之前提到的例子的激活函数是:
$$
f(z)=\left\{\begin{matrix}
 1&z\geqslant0 \\
 0&z<0
\end{matrix}\right.
$$

现在最常用的激活函数是sigmoid函数和softmax函数：
$$
\begin{array}{cc}
sigmoid(z)&=&\frac{1}{1+e^{-z}}\\
\\
softmax(z)&=&\frac{e^{z_i}}{\sum_je^{z_j}}
\end{array}
$$



![](http://www.saedsayad.com/images/ANN_Unit_step.png)
![](https://sebastianraschka.com/images/faq/logisticregr-neuralnet/sigmoid.png)

对比一下$f(z)$和$\phi(z)$ 两个函数的图像，很容易看出$\phi(z)$只是把直线拉弯了，让曲线变得光滑，这样就能求函数的梯度了。并且$\phi^{'}(z)=\phi(z)\phi(1-z)$，梯度对我们训练权重和偏置非常重要，之后会提到。

而$softmax(z)$函数可以看做$sigmoid(z)$函数的推广，$sigmoid(z)$函数一般做两类的分类，而$softmax(z)$可以做多个类别的分类。$relu(z)$和$hanh(x)$之后会在卷积神经网络和循环神经网络中介绍。

![](https://upload.wikimedia.org/wikipedia/commons/thumb/6/60/ArtificialNeuronModel_english.png/600px-ArtificialNeuronModel_english.png)

我们现在已经了解了神经网络中神经元的工作方式，他们将上一层所有神经元的值加权求和再加上一定偏置，然后用激活函数作用得到新的输出，作为下一层神经元的输入，直到最后一层，即输出层，输入结果$y^{'}$。

现在我们用公式来表现我们刚刚提到的联系。首先，我们定义 $a\_j^l$，为第 $l$层的第 $j$个激活函数， $w^l_{jk}$表示第 $l-1$层的第 $k$个神经元到第 $l$层的第 $j$个神经元的权重， $b^l\_j$表示第 $l$层的第 $j$个神经元。则第 $l$层的第 $j$个神经元与第 $l-1$层就通过 $a^l\_j$联系在一起：

$$a^l_j=\sigma(\sum_kw^l_{jk}a^{l-1}_k + b^l_j)$$

我们还可以把它们都向量化，用矩阵会大大提高运算速度。$a^l$表示第$l$层的所有激活函数值。同样的，$W^l$和$b^l$分别表示第$l$层的所有神经元的权重和偏置。之前你可能会奇怪为什么$w^l_{jk}$要这样写，$jk$反过来似乎更符合直觉，这是为了之后矩阵运算方便。
用矩阵则可简洁地表示为：
$$a^l=\sigma(W^la^{l-1} + b^l)$$

### 梯度下降
当我们对于每一个输入的$x$得到输出的结果$y^{'}$的时候，我们如何判断我们我们的结果怎么样呢？换句话说，我们如何判断$y^{'}$与$y$有多相同呢？

一个直观的想法就是对于每一个结果计算他们之间的均方误差：
$$
Error = \frac{1}{n}\sum_i(y_i-y^{'}_i)^2
$$

这样我们就可以把$Error$当成我们的目标函数，也称为代价函数$C$，通过不断改变每个神经元的权重$w$和偏置$b$，以达到$C$的最小值。即：
$$
\min {C=\frac{1}{2n}\sum\left \|  y(x)-a^l(x)\right \|^2}
$$

运用我们运筹学中学过的非线性规划，我们可以用梯度下降(gradient descent)的方法求得最小值。而且$C$是二次函数，所以存在全局最小值。我们设置一个合适的下降速率，慢慢地我们总能达到最小值。

$$
\theta_{n+1}(w,b) = \theta_{n}(w,b)-\eta \nabla C(\theta_{n}(w,b))
$$

![](https://sebastianraschka.com/images/faq/closed-form-vs-gd/ball.png)

我们总结一下我们提到的内容。首先，对于对输入$x$的所有特征进行加权求和得到$z$，然后用激活函数$\sigma(z)$作用在上面，得到这一层的输入$a^l$，通过这一层的激活函数值$a^l$与下一层的激活函数值$a^{l+1}$之间的联系，一直到输出层，得到输出结果$y'$，通过计算误差函数$C$来不断改变$w$和$b$以尽量达到最小的误差函数$C$，这样我们就训练好了我们的神经网络。
